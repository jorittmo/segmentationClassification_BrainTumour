---
title: Automatic identification and classification of brain tumours
authors:
  - name: Jonathan Rittmo
    # thanks: Use footnote for providing further information about author (webpage, alternative address)---*not* for acknowledging funding agencies. Optional.
    #department: Department of Statistics
    affiliation: University of Gothenburg
    location: Gothenburg, Sweden
    # email: hippo@cs.cranberry-lemon.edu
abstract: |
    Medical image analysis is an increasingly important concept for aiding
    patient diagnosis. But it is a tedious and time consuming task relying on subjective (albeit expert)
    knowledge of clinicians and technicians. Decision support systems
    relying on standardised analysis pipelines and classification
    could greatly aid and speed up these processes. Brain tumours are
    one of the more important diseases when it comes to early classification 
    and treatment selection. In this project I propose
    an analysis pipeline for automatic segmentation unhealthy tissue, based on
    a dataset of 3064 MR images from tumour patients with 3 different
    kinds of tumours. The pipeline consists of three parts: preprocessing,
    segmentation and classification. The aim of the project is to see whether
    automatic segmentation using a residual neural net can improve classification of
    extracted features using support vector machines, compared to classification of features extracted from
    full images. Results indicate that the proposed segmentation yields a
    better accuracy in the classification part than using full images. However, 
    the improvement is only slight. The small dataset is a limitation of the 
    study and future research with more data should be performed to solidify
    any potential effects.
keywords:
   - Brain tumor classificaton
   - Image segmentation
   - Support vector machines
   - ResNet
bibliography: references.bib
csl: apa.csl
link-citations: true
output:
    bookdown::pdf_book:
      base_format: rticles::arxiv_article
      keep_tex: false
header-includes: |
  \usepackage{setspace}\setstretch{1.3}
  \usepackage{float}
  \floatplacement{figure}{H}
  \usepackage{amsmath}
  \usepackage{bm}
  \usepackage{amsfonts}
  \usepackage{longtable}
  \usepackage{caption}
  \usepackage{pbox}
  \usepackage{booktabs}
  \usepackage{graphicx}
  \usepackage{cleveref}
  \renewcommand{\eqref}{\Cref}
  \Crefformat{equation}{#2#1#3}
#[colorlinks=true,linkcolor=black,anchorcolor=black,citecolor=black,filecolor=black,menucolor=black,runcolor=black,urlcolor=black] set this in hyperref in latex
---
```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# library(singcar)
# library(tidyverse)
# library(patchwork)
# library(ggpattern)
library(DiagrammeR)
library(imager)
library(magick)
library(R.matlab)
library(tidyverse)
library(kableExtra)
library(patchwork)
library(caret)
```

# Introduction

Decision support systems that facilitate clinical diagnoses are becoming more
and more important in health care systems. In this project I will focus
on classification of the brain tumour types: meningioma, glioma and pituitary
tumour. A tumour is a lump of cells characterised by its uncontrolled growth.
These lumps can be either malignant or benign where the malignant tumours are
recognised through their heterogenous shapes. All of these tumour types can
be malignant, but meningioma and pituitary tumours are generally benign.
Glioma on the other hand make up 80% of all malignant tumours in adults
[@chenMalignantGliomaLessons2012]. 
Detection of malignant brain tumours at an early stage is essential
for treatment selection and the survival rate of the patient and
hence have become an important issue in clinical image analysis. 
So the issue of segmentation of MR images have been considered thoroughly.
There are many exisiting techniques, where support vector machines (SVM)
and neural nets are some of the more popular choices of models 
[e.g. @alfonseAutomaticClassificationBrain2016; @damodharanCombiningTissueSegmentation2015;
@guoTumorDetectionMR2011; @torheimClassificationDynamicContrast2014].

Segmentation is the process of clustering the pixels of an image into $k$
classes dependent on some shared properties. For the removal of the skull in MR
images one must segment out the background which in the present dataset, due to
the quality and uniform background, could be done with simple thresholding.
Extracting tumour tissue on the other hand requires pixels in images to be classified either as
tumour tissue or healthy tissue [@abdel-maksoudBrainTumorSegmentation2015;
@ainFuzzyAnisotropicDiffusion2014]. This is a more difficult task since the
intensities of the tissue (healthy or unhealthy) pixels are more alike than 
tissue and background. And even though SVMs are popular
for this kind of task I will use a residual neural net [@heDeepResidualLearning2016]
for the segmentation and instead SVM for the tumour type classification task.

In my proposed analysis pipeline the essential parts involves: 
removal of non-nervous tissue through thresholding, segmentation of infected tissue
using residual neural nets (ResNet), feature extraction (first and second order statistics)
from the segmented regions and classification of these features into the categories "meningioma", "glioma"
and "pituitary". The proposed pipeline is summarised in \@ref(fig:pipeline).

```{r pipeline, out.width="50%", out.height="50%", fig.align='center', fig.cap="Proposed analysis pipeline for automatic classificaton of tumours from MR images."}

DiagrammeR::grViz("
digraph graph2 {

graph [layout = dot]

# node definitions with substituted label text
node [shape = rectangle, width = 4, fillcolor = Biege, orientation = 0]
a [label = '@@1']
b [label = '@@2']
c [label = '@@3']
d [label = '@@4']
e [label = '@@9']

node [shape = plaintext, orientation = 0, fillcolor = linen]
sa [label = '@@5'] 
sb [label = '@@6'] 
sc [label = '@@7']
sd [label = '@@8']


a -> b -> c -> d -> e

sa -> sb -> sc -> sd

b -> sa

sd -> c

}

[1]: paste0('Preprocessing (resizing/contrast enhancement)')
[2]: paste0('Skull stripping')
[3]: paste0('Segmentation using ResNet')
[4]: paste0('Feature extraction')
[5]: paste0('Thresholding to binary')
[6]: paste0('Morphological noise reduction')
[7]: paste0('Fill holes in the the binary image')
[8]: paste0('Erode the outer pixels and mask original')
[9]: paste0('Classification using SVM')
")

```

# Analysis

## Data

The data for this project was 3064 T1 weighted 512x512 magnetic resonance images from
233 patients with three types of tumours: meningioma, glioma and pituitary
tumours. These were resized to 256x256 to speed up computation using
nearest-neighbour interpolation.
The data has been used in @chengEnhancedPerformanceBrain2015 and
@chengRetrievalBrainTumors2016 and contains ground truth masks of tumour
regions generated by manual delineation of tumour regions.
There are two problems with this dataset: first the data is grouped on
patient level with varying number of observations per patient
and second it is imbalanced with the respect to number of images and
type of tumour. The grouped data would probably not be
problematic if this was purely a segmentation task but because we want to 
classify each tumour, it is important to ensure that no patient appears both
in train and test data simultaneously. 
It should also be noted that the dataset is not
really imbalanced with respect to classes if you take the grouping
into account, this can be seen in Figure \@ref(fig:imb) where there is 
a quite hefty skew between the classes when we look at the observation 
level but just a small skew disfavouring pituitary tumours.

```{r imb, fig.cap="Imbalance of dataset.", out.width="100%"}
library(ggsci)
nobs_class <- c(Meningioma = 708, Glioma = 1426, Pituitary = 930)
npats_class <- c(Meningioma = 82, Glioma = 89, Pituitary = 62)

x <- data.frame(Number = c(npats_class,nobs_class), Level = rep(c("Patient", "Observation"), each = 3), 
                Tumour = rep(c("Meningioma", "Glioma", "Pituitary"), 2)) %>% 
  mutate(perc = c(35.2, 38.2, 26.6, 23.1, 46.5, 30.4)) %>% arrange(Number)

ggplot(x, aes(y = Number, x = reorder(Tumour, -Number), fill = Level)) +
  geom_col(position = "dodge") +
  geom_text(aes(label = paste0(perc,"%")), position = position_dodge(0.9), vjust = -0.14, color = "black")+
  xlab("Tumour") +
  ylab("Count") +
  theme_bw() +
  theme(legend.position = "top") +
  scale_fill_jama()


```

When partitioning the data into train and test sets for the segmentation
task no consideration was taken to the classes of tumours, but the data
was partitioned with consideration to patient ID. For the classification
task on the other hand the data was partitioned on
patient and stratified with regards to tumour class. A sample of the 
original images from the dataset can be seen in Figure \@ref(fig:originals).

```{r originals, fig.cap="Example images from the original dataset.", out.width="100%", cache=TRUE}
input <- image_read('originals/1.png')
for (i in 2:64){
  test = image_read(paste0('originals/', i, '.png'))
  input <- c(input, test)
}
# allFiles = list.files(path = "originals/", pattern = ".png", full.names = T)
# input = image_read(allFiles)

image_montage(input, geometry = 'x100+5+5', tile = '8x8', bg = 'lightgray', shadow = TRUE)
```




## Preprocessing

One of the most important parts of medical image analysis of brain
tissue is the removal of non-nervous tissue such as the skull. This 
is usually referred to as skull stripping and several techniques are
available to do this. Morphology based skull stripping is one of the most
commonly used techniques and has proved to work well [@bensonMorphologyBasedEnhancement2014].
Hence it was the technique that I used for this project as well.
The overall processing pipeline can be seen in Figure \@ref(fig:pipeline). The images were first
normalised to a scale in the range [0, 1] and then I used
contrast-limited adaptive histogram equalization (CLAHE) to enhance contrasts 
of the images. Simple histogram equalisation
computes a single histogram for the entire image, but in contrast CLAHE computes several
histograms for different sections of the image [@10.5555/180895.180940]. In that way it improves local contrasts
and edges -- suitable for tumour detection. 

After this enhancement simple thresholding was very effective to segment out any
tissue from the background and create a binary mask due to the quality of the
dataset and the clear intensity differences. Area opening was then applied to
the mask to remove potential noise, i.e. removing connected objects with fewer
than 10 pixels and any holes in the main object (i.e. the skull) was filled. The
outer pixels were then eroded away using a disk shaped structuring element with
a 15 pixel radius. This mask was then applied to the gray-scale image to remove
the skull. Results of these operations on the images in \@ref(fig:originals) can
be seen in Figure \@ref(fig:skullstripped).

```{r skullstripped, fig.cap="Skull stripped and contrast enhanced example images.", out.width="100%", cache=TRUE}

allFiles = list.files(path = "sktr/", pattern = ".png", full.names = T)
input = image_read(allFiles)

image_montage(input, geometry = 'x100+5+5', tile = '8x8', bg = 'lightgray', shadow = TRUE)
```

## Segmentation

The segmentation was conducted by training a residual neural
net [@heDeepResidualLearning2016] on the ground truth images so that each pixel
in every image was given the label "tumour", "normal" or "background". The ground
truth for tumour has been manually delineated by professionals [@chengEnhancedPerformanceBrain2015]
and the "normal" pixels was segmented out by simple thresholding. Residual
neural nets have been used extensively in semantic segmentation tasks and proven
effective in tumour detection [e.g. @zeineldinDeepSegDeepNeural2020]. I created
a DeepLab v3+ convolutional neural network based on ResNet-18 which is an 18
layer deep pre-trained residual neural net, in Matlab. DeepLab v3+ is a network
used for semantic segmentation and basing it on a predefined network adds a few
additional layers to it (i.e. to ResNet-18) that are set up for the specific
pixel classification and image input. The data was partitioned 60/20/20
for training, validation and testing in such a way that no patient
appeared in either set simultaneously. 

```{r pixhist, fig.cap="Pixel frequency of the three classes.", out.width="100%"}
x <- data.frame(Frequency = c(0.0166, 0.3583, 0.6251), Tissue = c("Tumour", "Normal", "Background"))
ggplot(x, aes(y = Frequency, x = Tissue)) +
  geom_col(position = "dodge") +
  theme_bw() +
  scale_fill_jama()

```

Due to both background and normal tissue
pixels being overrepresented in the images as seen in Figure \@ref(fig:pixhist)
I balanced the classes by weighting them relative to the frequency of the normal
tissue. So the weight of "tumour" was $0.3583/0.0166 = 21.6205$ and so on.
This effectively punish the network more for misclassifying a "tumour" pixel
than "background" or "normal". The network was trained using stochastic gradient descent with momentum
optimisation with weight decay (L2 regularisation) and early stopping to avoid over-fitting. Some
evaluation metrics of the network can be seen in Table \@ref(tab:semtab).


```{r semtab}

x <- data.frame(Tumour = c(0.8401,    0.3855,    0.2208),
                Normal = c(0.9427,    0.9239,    0.8041),
                Background = c(0.9923,    0.9894,    0.9821))
rownames(x) <- c("Accuracy", "IoU", "MeanBFScore")
kable(t(x), align = "r", digits = 3, booktabs = T, 
      format = "latex", caption = "Evaluation metrics of semantic segmenation on test data")%>%
  kable_styling(latex_options = c("hold_position"))
```


IoU refers to the correctly classified pixels divided by the sum of false positives and
false negatives and the mean BF score refers to how well the predicted
boundary of an object aligns with the ground truth boundary. And as can be seen
neither of these scores are very good for the "tumour" class, while the accuracy
still is (moderatly) high. This indicates that the tumour regions predicted by the network
probably are much larger than the ground truth tumour. In Figure \@ref(fig:segtest)
the first 64 segmented images in the test set are shown where the yellow represents
ground truth tumour and the non-coloured areas represent the predicted tumour region.
As noted it seems like the network often classify much larger areas than the ground truth.

```{r segtest, fig.cap="Segmented images from test set.", out.width="100%", cache=TRUE}
input <- image_read('segmented_test_images/1.png')
for (i in 2:64){
  test = image_read(paste0('segmented_test_images/', i, '.png'))
  input <- c(input, test)
}
# allFiles = list.files(path = "segmented_test_images/", pattern = ".png", full.names = T)
# input = image_read(allFiles)

image_montage(input, geometry = 'x100+5+5', tile = '8x8', bg = 'lightgray', shadow = TRUE)
```

Since I am going to use the predicted tumour areas as masks for feature
extraction the irrelevant false positive pixels (i.e. the larger areas
classified as tumour) might affect later classification of tumour type.
However, @chengEnhancedPerformanceBrain2015 show that using the surrounding
tissue of tumours can aid classification due to the fact that they often grow in
different areas of the brain and hence I went on with the analysis. 

## Tumour classification

For classifying type of tumour the segmented tumour areas were used as masks to
hide any irrelevant information when extracting features (eight of the 3064
tumours were not detected by the network at all so no mask was used for these
images). Preferably classification of tumour would have been done only on the
images in the test dataset from the segmentation task. However, due to the small
dataset, the predicted tumour areas from the training images were used as well.
It is common to use features based on Wavelet transformation when classifying
magnetic resonance images. However, @aggarwalFirstSecondOrder2012 found that
first and second order statistical features yielded better results. Hence, the
extracted first order features were: mean, variance, skewness and kurtosis. In
addition Hu's seven invariant moments [@huVisualPatternRecognition1962] were
extracted to represent the shape of the tumours. Unfortunately these moments are
mostly appropriate if we have a very clean segmentation of the objects of
interest which, as noted and displayed in Figure \@ref(fig:segtest), is not the
case for many of the images in this dataset. However, due to the discriminatory
power of these moments when objects are segmented properly these moments they
were included anyway.

The extracted second order features were correlation, energy and homogeneity.
Contrast is also a commonly used second order feature but since it is basically
the same thing as variance it was left out of the analysis. These features
are calculated from a normalised gray-level spatial dependence matrix, which is a 
matrix with counts of how often a pixel with grey-level intensity $I_1$
occurs horizontally adjacent to a pixel with grey-level intensity $I_2$.
In this analysis the grey-levels of the image were binned to 8 values
so that the gray-level spatial dependence matrix was of size 8x8.
Correlation is a measure of the linear dependency between a pixel and its neighbour, 
for all the pixels in the image. Energy is the summation of the squared elements
in the normalised spatial dependence matrix and homogeneity is a measure of 
the closeness to the diagonal of the distribution of the elements in the
spatial dependence matrix. That is, if no pixel of grey-level intensity $I_1$
appeared adjacent to any pixel of grey-level intensity $I_2$ the spatial dependence
matrix would be a diagonal matrix and the homogeneity would be 1.

So, these features were extracted from the areas predicted by the segmentation
model and then fed into a support vector machine (SVM) with a Gaussian kernel.
SVM was the model of choice because it has been used extensively for these types 
of classification problems [e.g. @bahadureImageAnalysisMRI2017; @aggarwalFirstSecondOrder2012]
with good results. After training the model had a ten fold cross validation error
of $0.1877$ and a test error of $0.1794$ indicating that the model can generalise
to some degree even if a better accuracy would have been desirable. To have something
to compare against I also trained the same model on features extracted from the full
images (i.e. the skull stripped images without tumour masking). This model got a ten
fold cross validation error of $0.1914$ and a test error of $0.2292$. So the segmentation
seems to improve performance somewhat even if the improvement is slight. To get an intuition of
the possibly better discriminatory power when using segmented images one can observe Figure
\@ref(fig:pca) where the features have been reduced to the first two principal components.
Interestingly we can see that the most separable classes are "glioma" and "pituitary"
which is consistent with the mentioned fact that pituitary tumours generally are benign,
gliomas make up ~80% of the malignant tumours in adults and malignant tumours are characterised
by their heterogenous shapes. 
<!-- THIS IS AFTER COMMENTS  -->

We do however not know the malignancy of the tumours in this dataset. But if we
merge the meningiomas and pituitary tumours and call this class "generally
benign tumours" and call the gliomas "generally malignant tumours" and train a
model with these labels we achieve a ten fold cross validation and test error
$0.1050$ and $0.0774$ on the segmented images compared to a ten fold cross
validation and test error of $0.1520$ and $0.2170$ respectively. Further
indicating that segmentation of the images, at least to predict malignancy,
is helpful.


```{r pca, fig.cap="Dimension reduced feature space from segmented and full images.", out.width="100%"}

segfeat <- R.matlab::readMat('hu_central_glcm_segimage.mat')
segfeat <- segfeat[["feat"]]
lab <- readMat('label.mat')
lab <- lab[["label"]]
lab <- lab[complete.cases(segfeat)]
segfeat <- segfeat[complete.cases(segfeat), ]
segpca <- princomp(segfeat)
segpca <- data.frame(segpca[["scores"]][ , 1:2], lab = lab, image = "Segmented")
segpca <- segpca[!segpca[,1] %in% boxplot.stats(segpca[, 1])$out, ]

fullfeat <- R.matlab::readMat('hu_central_glcm_fullimage.mat')
fullfeat <- fullfeat[["feat"]]
lab <- readMat('label.mat')
lab <- lab[["label"]]
lab <- lab[complete.cases(fullfeat)]
fullfeat <- fullfeat[complete.cases(fullfeat), ]
fullpca <- princomp(fullfeat)
fullpca <- data.frame(fullpca[["scores"]][ , 1:2], lab = lab, image = "Full")
fullpca <- fullpca[!fullpca[,1] %in% boxplot.stats(fullpca[, 1])$out, ]

bothpca <- rbind(segpca, fullpca)

ggplot(bothpca, aes(x = Comp.1, y = Comp.2, color = factor(lab))) +
  geom_point(size = 0.4, alpha = 0.4) +
  facet_wrap(.~image, scales="free") +
  labs(x = "Component 1", y = "Component 2", col="Tumour", shape="Tumour")+
  scale_color_manual(label = c("Meningioma", "Glioma", "Pituitary"),  values = c(1, 2, 3)) +
  scale_shape_manual(label = c("Meningioma", "Glioma", "Pituitary"),  values = c(1, 2, 3)) +
  theme_bw() +
  theme(legend.position = "top")
```


```{r conftab}

tstpred <- R.matlab::readMat("tstpred.mat")
tstpred <- factor(tstpred[["tstpred"]], labels = c("Meningioma", "Glioma", "Pituitary"))

testlab <- R.matlab::readMat("testLab.mat")
testlab <- factor(testlab[["testLab"]], labels = c("Meningioma", "Glioma", "Pituitary"))


results <- confusionMatrix(testlab, tstpred)
x <- as.matrix(results, what = "classes")

x <- x[-7, ]

kable(x, align = "r", digits = 3, booktabs = T, 
      format = "latex", caption = "Evaluation of class-wise predictions on segmented test data.")%>%
  kable_styling(latex_options = c("HOLD"))
```

As can be seen the feature separability is somewhat increased for the segmented
images even though the effect might be slight. Furthermore, Figure
\@ref(fig:pca) seems to indicate that Meningioma is the most difficult class to
discriminate. Looking at class-wise prediction performance on test data in Table
\@ref(tab:conftab), this is obviously the case. The measures are calculated by
comparing each class to the remaining classes and their definitions can be find
in appendix \@ref(app). As can be seen Meningioma has the lowest scores on all measures
except for the negative predictive value (i.e. the probability that a tumour is
not Meningioma if it has been classified as anything else). However, both the
positive and negative predictive values are heavily dependent on the prevalence
(base rate) of the classes which we can see is imbalanced in the test set. The
reason for this imbalance even though the the training and testing set were
stratified on tumour class is probably that the data also were partitioned on
patient and each patient had a varying number of images. Therefore exactly equal
ratios of the classes in test and training data were difficult to achieve.

# Discussion and conclusion

This project has looked at methodology to automatically classify MR images from brain cancer
patients into three different tumour types. This analysis was done in three major steps:
preprocessing, segmentation and classification. The preprocessing, with skull stripping
being the most important operation to automatise, was in general successful. The segmentation
on the other hand had a more ambiguous result. For some images it worked great but overall
it had a high false positive rate. Extracting shape features based on dirty segmentation
is generally not a good idea and as we saw for the classification part of the project
the result for segmented images was only marginally better than using the full. Another
limitation of this project was the small dataset used. Preferably, the tumour classification
should only have been performed on the test set from the segmented images to fully evaluate
performance of this analysis pipeline but due to the limitations of such a small dataset
I chose to perform the classification on the segmented images from the train set as well.
Hence, the results of the study might not be fully generalisable and more research is encouraged.
All in all, however, the slight classification improvement of the segmented images shows
that using automatic segmentation pipelines could aid the classification and diagnosis
of brain tumours. 




# References {-}

<div id="refs"></div>

# (APPENDIX) Appendix {-}

# Appendix A {#app}

Given a two-class classification problem and the confusion matrix:
\begin{table}[H]
\centering
\caption{Confusion matrix for a two-class problem.}
\begin{tabular}{l|l|l|l}
\hline
\multicolumn{2}{l}{\multirow{2}{*}{}}    & \multicolumn{2}{c}{Predicted class} \\ \cline{3-4}
\multicolumn{2}{l}{}                     & Class 1             & Class 2            \\ 
\hline
\multirow{2}{*}{True class} & Class 1 & A & B \\ \cline{2-4}
                          & Class 2       & C  & D \\
\hline
\end{tabular}
\label{tab:errortab}
\end{table}

Then the measurements in Table \@ref(tab:conftab) are defined as:
$$
\text{Sensitivity} = \frac{A}{(A+C)}
$$
$$
\text{Specificity} = \frac{D}{(B+D)}
$$
$$
\text{Prevalence} = \frac{(A+C)}{(A+B+C+D)}
$$

$$
\text{PPV} = \frac{\text{sensitivity} * \text{prevalence}}{(\text{sensitivity}*\text{prevalence}) + [(1-\text{specificity})(1-\text{prevalence})]}
$$
$$
\text{NPV} = \frac{\text{specificity} * (1-\text{prevalence})}{[(1-\text{sensitivity})*\text{prevalence}] + [(\text{specificity})(1-\text{prevalence})]}
$$
$$
\text{Detection Rate} = \frac{A}{A+B+C+D}
$$
$$
\text{Detection Prevalence} = \frac{A+B}{A+B+C+D}
$$
$$
\text{Balanced Accuracy} = \frac{\text{sensitivity}+\text{specificity}}{2}
$$
$$
\text{Precision} = \frac{A}{A+B}
$$
$$
\text{Recall} = \frac{A}{A+C}
$$

However, note that since the classification in this project was a multi-class problem 
the measurements are calculated by comparing one class to the all the other.